# End-to-End Data Pipeline for Santa Luc√≠a Lottery: Historical Data Mining, Web Scraping, ETL, and Dynamic Visualization

## **Description:** 

In this project I tried to answer and discover insights about **"Loteria Santa Lucia de Guatemala"** which is the biggest lottery in my country (Guatemala). Also to create a historical dataset for their winnig number, due to the fact that there is no way to retrieve the old data from the past draws.

## **Table of Contents**
1. [Description](#description)
2. [Why of this project?](#why-of-this-project)
3. [Automated ETL Process for Loteria Santa Lucia Data](#automated-etl-process-for-loteria-santa-lucia-data)
   - [ETL Architecture](#etl-architecture)
   - [Extract Phase](#extract-phase)
   - [Transform Phase](#transform-phase)
   - [Results](#results)
   - [Future Steps](#future-steps)
4. [Requisites](#requisites)
5. [Insights and Findings from Visualizations EDA 2024 - 01/2025](./eda_2024-01_2025.md)
6. [Technologies and Tools Used](#technologies-and-tools-used-üõ†Ô∏è)
7. [Project Structure](#project-structure)
8. [Next Steps](#next-steps)
9. [Acknowledgements](#acknowledgements)


## **Why of this project?** [Go Back ‚¨ÜÔ∏è](#table-of-contents)

Loter√≠a Santa Luc√≠a is the largest and oldest lottery in Guatemala, founded in 1956. Unfortunately, there is no way to retrieve historical data other than through old physical newspapers, some Facebook videos (available only since 2018), or by purchasing old newspapers (PDFs) from the "National Newspaper Archive of Guatemala," which is very expensive (I tried it...).

Once a draw expires, the data is permanently erased. There is no way to perform any kind of audit on "Loter√≠a Santa Luc√≠a." Surprisingly, no thesis projects or university studies from Math or Statistics students have been conducted on this topic. Additionally, there is no dataset available on platforms like Kaggle.

Due to all these factors, I found a valuable way to provide data that no one else has, which could be interesting for those interested in statistics and Machine Learning.


## **Automated ETL Process for Loteria Santa Lucia Data** [Go Back ‚¨ÜÔ∏è](#table-of-contents)

### Introduction

This project focuses on automating the ETL (Extract, Transform, Load) process for Santa Lucia Lottery data. The main goal is to efficiently collect, clean, and store data to enable analysis and visualization, highlighting insights such as winning patterns, frequently rewarded locations, and more.

### ETL Architecture

‚öôÔ∏è ETL Architecture  (updated 2025-06)

The pipeline is now 100 % **serverless-ready** and consists of two main stages
(*extract* ‚Üí *transform & load*).  
Each stage will eventually run inside its own **AWS Lambda** function, but you can still execute them locally for development.


| Stage | What happens | Key AWS resources |
|-------|--------------|-------------------|
| **Extraction** | *extract.py* scrapes the Santa Luc√≠a site with headless Chrome ‚Üí saves a `.txt` per draw ‚Üí uploads it **twice**:<br>  ‚Ä¢ `raw/sorteo_<N>.txt` &nbsp;(_simple bucket_)<br>  ‚Ä¢ `raw/year=<YYYY>/sorteo=<N>/‚Ä¶` &nbsp;(_partitioned bucket_)<br>Skips draws that are already present. | *S3 (simple + partitioned)*<br>*Secrets Manager* (bucket names)<br>*VPC Gateway Endpoint* (S3) |
| **Transformation + Load** | *transformer.py* downloads any new `.txt`, splits **HEADER / BODY**, cleans with Pandas/pyarrow, and writes:<br>  ‚Ä¢ `premios_<N>.parquet`  & `sorteos_<N>.parquet` ‚Üí **simple bucket**<br>  ‚Ä¢ Hive-style `processed/premios/` & `processed/sorteos/` partitioned by `year` and `sorteo` ‚Üí **partitioned bucket** | *S3 (simple + partitioned)*<br>*Secrets Manager* |
| **Discovery + Query (serverless)** | A **Glue Crawler** runs on the partitioned bucket, registers tables `loteria.premios` & `loteria.sorteos`.<br>Analysts query with **Athena** or visualise in **QuickSight** without touching the VPC. | *AWS Glue Crawler*<br>*AWS Athena*<br>*AWS QuickSight* |

---

### üü¢ Extraction phase

| Item | Details |
|------|---------|
| **Tool** | Selenium + Python |
| **Script** | [`modules/ETL/extract.py`](modules/ETL/extract.py) |
| **Flow** | 1. Open awards page ‚Üí dismiss pop-up<br>2. Choose ID (or latest) ‚Üí click link<br>3. Parse header/body, infer real draw number & date<br>4. Write `results_raw_lottery_id_<ID>_<title>.txt`<br>5. Upload to both buckets (simple & partitioned) |
| **Idempotence** | Before scraping, the script checks `processed/year=<YYYY>/sorteo=<N>/sorteos.parquet`; if it exists, the draw is skipped. |

---

### üü¢ Transform + Load phase

| Item | Details |
|------|---------|
| **Tool** | Pandas, PyArrow, Boto3 |
| **Script** | [`modules/ETL/transformer.py`](modules/ETL/transformer.py) |
| **Flow** | 1. List raw `.txt` files in partitioned bucket<br>2. Skip draws already processed<br>3. Download ‚Üí split HEADER/BODY<br>4. Create two DataFrames:<br>   ‚Ä¢ **sorteos** (metadata + prize digits)<br>   ‚Ä¢ **premios** (ticket, letters, amount, vendor, city, depto.)<br>5. Write Parquet to **simple bucket** (`premios_<N>.parquet`, `sorteos_<N>.parquet`)<br>6. Write partitioned Parquet (`processed/premios/`, `processed/sorteos/`) to **partitioned bucket** |
| **Data model** | Columns are strongly typed; extra columns (`year`, `sorteo`) are added before partition write. |

---

### üü¢ Resulting benefits

* **Dual-bucket strategy** ‚Üí quick ad-hoc analysis (simple) **and** scalable lakehouse queries (partitioned).  
* **Glue Crawler + Athena** ‚Üí zero-admin SQL layer, cheap (< $5/TB scanned).  
* **QuickSight dashboards** ‚Üí shareable insights without moving data out of AWS.  
* **Ready for Lambda/Step Functions** ‚Üí push-button or cron-based automation.

---

> **Note:** The old `loader.py` (MySQL/RDS) has been retired to keep the stack fully serverless and cost-efficient. Historic diagrams and EDA from 2024‚Äì2025 have been moved to [`/docs/eda_2024-01_2025.md`](docs/eda_2024-2025.md).


### Conclusion

This automated ETL project demonstrates expertise in data extraction, transformation, and storage while showcasing potential for advanced analytics and visualization. It is a robust solution for managing lottery data efficiently.


## **Technologies and Tools Used üõ†Ô∏è** [Go Back ‚¨ÜÔ∏è](#table-of-contents)

### üêç Languages and Python Libraries
- **Python 3.12** ‚Äì Core language for the entire ETL pipeline.
  - **Selenium** ‚Äì For headless web scraping and automation.
  - **Pandas** ‚Äì For data cleaning, formatting, and transformation.
  - **PyArrow** ‚Äì For writing Parquet files efficiently to S3.
  - **Boto3** ‚Äì AWS SDK for Python; used for interacting with S3, Secrets Manager, and more.
  - **re / json / os** ‚Äì Standard libraries used in extraction and transformation.

### ‚òÅÔ∏è AWS Cloud Services
- **Amazon S3** ‚Äì Dual-storage strategy:  
  - Simple bucket (flat files for EDA)  
  - Partitioned bucket (Hive-style for Athena/QuickSight)
- **AWS Secrets Manager** ‚Äì Secure retrieval of S3 bucket names and credentials.
- **AWS Glue Crawler** ‚Äì Automatically detects and registers schema + partitions for Athena.
- **Amazon Athena** ‚Äì Serverless SQL engine to query Parquet data stored in S3.
- **Amazon QuickSight** ‚Äì For cloud-based dashboards and storytelling.
- **(Planned)** **AWS Lambda** ‚Äì Will run each ETL step serverlessly.
- **(Planned)** **AWS Step Functions / EventBridge** ‚Äì For orchestration of the full pipeline.

### üíª Development & Runtime Environment
- **Jupyter Notebooks (SageMaker Studio)** ‚Äì For exploratory data analysis and chart prototyping.
- **Terraform** ‚Äì Infrastructure-as-code used to provision S3, IAM, SageMaker, Glue, etc.
- **ChromeDriver + Headless Chrome** ‚Äì For scraping the official lottery website.

### üìä Visualization & Analytics
- **Matplotlib / Seaborn** ‚Äì Used for generating visual insights during EDA.
- **AWS QuickSight** ‚Äì (Active phase) for producing clean, dynamic dashboards.
- **(Optional)** Dash / Streamlit ‚Äì Considered for future real-time visualizations.

### ‚öôÔ∏è Methods & Design Patterns
- **Serverless-first architecture** ‚Äì All core services designed to run without persistent compute.
- **Dual Storage Strategy** ‚Äì Separate S3 buckets for flat (notebook) and partitioned (analytics) use cases.
- **Partitioned Data Lake** ‚Äì Optimized S3 structure with `year=/sorteo=` folders.
- **Modular Python code** ‚Äì Scripts are atomic and Lambda-compatible.
- **Secure-by-default** ‚Äì All sensitive config (bucket names, keys) are managed with Secrets Manager.


## üîó Reference Documentation

Below are official AWS documentation links for key services and methods used in this project:

- **Glue Crawlers and Partitioned Data Lakes**
  - [Incremental Crawls for Partitioned Data in Glue](https://docs.aws.amazon.com/glue/latest/dg/incremental-crawls.html)
  - [Adding a Glue Crawler](https://docs.aws.amazon.com/glue/latest/dg/add-crawler.html)
- **S3 Partitioning Strategy**
  - [Hive-style Partitioning in AWS Glue](https://docs.aws.amazon.com/glue/latest/dg/partitioned-data.html)
- **Athena and Querying Parquet**
  - [Using Athena with Partitioned Tables](https://docs.aws.amazon.com/athena/latest/ug/partitioning.html)
- **IAM and Secrets Management**
  - [Managing Secrets with AWS Secrets Manager](https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html)
- **Infrastructure-as-Code**
  - [Terraform AWS Provider Docs](https://registry.terraform.io/providers/hashicorp/aws/latest/docs)


## üß† Architectural Decisions

- [ADR-001: Separation of VPC and Serverless Services](./vpc-separation.md)



## üöß Challenges Faced and How They Were Overcome

Take a look to the challanged I faced and how I overcome them by giving a look to this file:

- [Challanges Faced and How They Were Overcome](./challanges_faced.md)